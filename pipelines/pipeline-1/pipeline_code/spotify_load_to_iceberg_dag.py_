"""
Load Spotify JSONL to Iceberg Bronze - Using DuckDB native S3 reading.
"""

from __future__ import annotations

import logging
from datetime import timedelta
from typing import List

import pendulum
from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowException
from pyiceberg.partitioning import PartitionSpec, PartitionField
from pyiceberg.transforms import DayTransform

from libs.loaders.iceberg_loader import IcebergLoader
from libs.transformers.spotify_transformer import SpotifyTransformer
from libs.duckdb_config import query

log = logging.getLogger(__name__)

# ============================================================================
# Configuration
# ============================================================================

BRONZE_TABLE = "bronze.spotify_tracks"
S3_BASE_PATH = "s3://inbound/raw/spotify/api/tracks"

default_args = {
    'owner': 'data-engineering',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# ============================================================================
# Tasks
# ============================================================================

def check_files_exist(**context):
    """Check if JSONL files exist for the execution date"""
    execution_date = context['execution_date']
    run_id = context['run_id']
    ti = context['task_instance']
    
    # Determine dates to load
    if run_id and run_id.startswith("manual__"):
        # Manual run: last 7 days
        dates = [
            (execution_date - timedelta(days=i)).format('YYYY-MM-DD')
            for i in range(7)
        ]
    else:
        # Scheduled run: just execution_date
        dates = [execution_date.format('YYYY-MM-DD')]
    
    log.info(f"Checking files for dates: {dates}")
    
    # Use DuckDB to check file existence
    file_patterns = [f"{S3_BASE_PATH}/{date}.jsonl.gz" for date in dates]
    
    existing_files = []
    for pattern in file_patterns:
        try:
            # Try to read just 1 row to verify file exists
            result = query(f"SELECT COUNT(*) as cnt FROM read_json_auto('{pattern}') LIMIT 1")
            if result['cnt'].iloc[0] >= 0:  # File readable
                existing_files.append(pattern)
                log.info(f"✅ Found: {pattern}")
        except Exception as e:
            log.info(f"⏭️  Not found: {pattern} ({e})")
    
    if not existing_files:
        log.warning("No files found")
        ti.xcom_push(key='files_exist', value=False)
        ti.xcom_push(key='dates_to_load', value=[])
        return
    
    ti.xcom_push(key='files_exist', value=True)
    ti.xcom_push(key='dates_to_load', value=dates)
    
    log.info(f"✅ Found {len(existing_files)} files")


def load_to_bronze(**context):
    """
    Load JSONL to Bronze using DuckDB's native S3 reading.
    """
    ti = context['task_instance']
    
    files_exist = ti.xcom_pull(task_ids='check_files', key='files_exist')
    dates_to_load = ti.xcom_pull(task_ids='check_files', key='dates_to_load')
    
    if not files_exist:
        log.info("No files to load")
        return {'status': 'skipped', 'records': 0}
    
    log.info(f"Loading data for dates: {dates_to_load}")
    
    # Get transformation SQL
    transformer = SpotifyTransformer()
    if len(dates_to_load) == 1:
        # Single date - use glob pattern
        sql = transformer.get_bronze_query(
            s3_path=f"{S3_BASE_PATH}/{dates_to_load[0]}.jsonl.gz"
        )
    else:
        # Multiple dates - specify files
        sql = transformer.get_bronze_query_for_dates(dates_to_load)
    
    log.info("Executing DuckDB transformation...")
    log.debug(f"SQL: {sql[:500]}...")  # Log first 500 chars
    
    # Load to Iceberg
    loader = IcebergLoader()
    
    # Define partitioning
    partition_spec = PartitionSpec(
        PartitionField(
            source_id=1,
            field_id=1000,
            transform=DayTransform(),
            name="played_date_day"
        )
    )
    
    result = loader.load_incremental_from_sql(
        sql=sql,
        table_identifier=BRONZE_TABLE,
        unique_key="played_at",  # Deduplicate on timestamp
    )
    
    log.info(f"✅ Bronze load complete: {result}")
    
    ti.xcom_push(key='load_result', value=result)
    return result


def validate_load(**context):
    """Validate the Bronze load"""
    ti = context['task_instance']
    
    files_exist = ti.xcom_pull(task_ids='check_files', key='files_exist')
    
    if not files_exist:
        log.info("No files to validate")
        return
    
    load_result = ti.xcom_pull(task_ids='load_to_bronze', key='load_result')
    
    if not load_result or load_result['status'] != 'success':
        raise AirflowException("Bronze load failed")
    
    # Optional: Query the table to verify
    row_count_query = f"""
        SELECT COUNT(*) as total_rows
        FROM iceberg_scan('s3://warehouse/bronze/spotify_tracks')
    """
    
    try:
        result = query(row_count_query)
        total_rows = result['total_rows'].iloc[0]
        log.info(f"✅ Validation passed. Total rows in Bronze: {total_rows}")
    except Exception as e:
        log.warning(f"Could not verify row count: {e}")
    
    return load_result


# ============================================================================
# DAG
# ============================================================================

with DAG(
    dag_id="spotify_load_to_iceberg",
    default_args=default_args,
    description="Load Spotify JSONL to Iceberg Bronze (DuckDB-powered)",
    schedule_interval="*/30 * * * *",
    start_date=pendulum.datetime(2025, 1, 1, tz="UTC"),
    catchup=False,
    max_active_runs=1,
    tags=["spotify", "iceberg", "bronze", "duckdb"],
) as dag:
    
    check_task = PythonOperator(
        task_id='check_files',
        python_callable=check_files_exist,
    )
    
    load_task = PythonOperator(
        task_id='load_to_bronze',
        python_callable=load_to_bronze,
    )
    
    validate_task = PythonOperator(
        task_id='validate_load',
        python_callable=validate_load,
    )
    
    check_task >> load_task >> validate_task